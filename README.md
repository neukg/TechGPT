


# TechGPT: Technology-Oriented Generative Pretrained Transformer
Demo: [TechGPT-neukg](http://techgpt.neukg.com) <br>
HuggingFaceğŸ¤—: [neukg/TechGPT-7B](https://huggingface.co/neukg)

<div align="center">

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/neukg/TechGPT/blob/main/LICENSE)
[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/neukg)
</div>


## ç®€ä»‹ Introduction
TechGPTæ˜¯[â€œä¸œåŒ—å¤§å­¦çŸ¥è¯†å›¾è°±ç ”ç©¶ç»„â€](http://faculty.neu.edu.cn/renfeiliang)å‘å¸ƒçš„å‚ç›´é¢†åŸŸå¤§è¯­è¨€æ¨¡å‹ã€‚<br>
ä¸»è¦å¼ºåŒ–äº†å¦‚ä¸‹ä¸‰ç±»ä»»åŠ¡:
- ä»¥â€œçŸ¥è¯†å›¾è°±æ„å»ºâ€ä¸ºæ ¸å¿ƒçš„å…³ç³»ä¸‰å…ƒç»„æŠ½å–ç­‰å„ç±»ä¿¡æ¯æŠ½å–ä»»åŠ¡
- ä»¥â€œé˜…è¯»ç†è§£â€ä¸ºæ ¸å¿ƒçš„å„ç±»æ™ºèƒ½é—®ç­”ä»»åŠ¡ã€‚
- ä»¥â€œæ–‡æœ¬ç†è§£â€ä¸ºæ ¸å¿ƒçš„å…³é”®è¯ç”Ÿæˆç­‰å„ç±»åºåˆ—ç”Ÿæˆä»»åŠ¡ã€‚

åœ¨è¿™ä¸‰å¤§è‡ªç„¶è¯­è¨€å¤„ç†æ ¸å¿ƒèƒ½åŠ›ä¹‹å†…ï¼ŒTechGPTè¿˜å…·å¤‡äº†å¯¹è®¡ç®—æœºç§‘å­¦ã€ææ–™ã€æœºæ¢°ã€å†¶é‡‘ã€é‡‘èå’Œèˆªç©ºèˆªå¤©ç­‰åä½™ç§å‚ç›´ä¸“ä¸šé¢†åŸŸè‡ªç„¶è¯­è¨€æ–‡æœ¬çš„å¤„ç†èƒ½åŠ›ã€‚
ç›®å‰ï¼ŒTechGPTé€šè¿‡æç¤ºå’ŒæŒ‡ä»¤è¾“å…¥æ–¹å¼çš„ä¸åŒï¼Œæ”¯æŒå•è½®å¯¹è¯å’Œå¤šè½®å¯¹è¯ï¼Œæ¶µç›–äº†é¢†åŸŸæœ¯è¯­æŠ½å–ã€å‘½åå®ä½“è¯†åˆ«ã€å…³ç³»ä¸‰å…ƒç»„æŠ½å–ã€æ–‡æœ¬å…³é”®è¯ç”Ÿæˆã€æ ‡é¢˜ç”Ÿæˆæ‘˜è¦ã€æ‘˜è¦ç”Ÿæˆæ ‡é¢˜ã€æ–‡æœ¬é¢†åŸŸè¯†åˆ«ã€æœºå™¨é˜…è¯»ç†è§£ã€åŸºç¡€å¸¸è¯†é—®ç­”ã€åŸºäºä¸Šä¸‹æ–‡çš„çŸ¥è¯†é—®ç­”ã€å»ºè®®å’¨è¯¢ç±»é—®ç­”ã€æ–‡æ¡ˆç”Ÿæˆã€ä¸­è‹±äº’è¯‘å’Œç®€å•ä»£ç ç”Ÿæˆç­‰å¤šé¡¹è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚

---
TechGPT mainly strengthens the following three types of tasks:
- Various information extraction tasks such as relation triplet extraction with "knowledge graph construction" as the core
- Various intelligent question-and-answer tasks centered on "reading comprehension".
- Various sequence generation tasks such as keyword generation with "text understanding" as the core.
Within these three core natural language processing capabilities, TechGPT also has the ability to process natural language texts in more than ten vertical professional fields such as computer science, materials, machinery, metallurgy, finance, and aerospace.
TechGPT supports single-round dialogue and multi-round dialogue through different input methods of prompts and instructions, covering domain term extraction, named entity recognition, relation triplet extraction, text keyword generation, title generation abstract, abstract generation title, text field Recognition, machine reading comprehension, basic common sense question and answer, context-based knowledge question and answer, suggestion consultation question and answer, copy generation, CM-EN translation and simple code generation and many other natural language understanding and generation capabilities.

## æ¨¡å‹åŠŸèƒ½ Model Capabilities

æ¨¡å‹åœ¨ä¿ç•™åŸºæœ¬çš„å¯¹è¯å’Œç¿»è¯‘èƒ½åŠ›ä¹‹å¤–ï¼Œé‡ç‚¹å¼ºåŒ–äº†åŸºäº**TechKG**çš„æ•°æ®å†…å®¹ï¼Œå› æ­¤ï¼Œ**TechGPT**æ›´æ“…é•¿åœ¨è®¡ç®—æœºç§‘å­¦ç­‰ä¸“ä¸šé¢†åŸŸï¼Œå®Œæˆå®ä½“åŠå®ä½“ç±»å‹æŠ½å–ã€å…³ç³»ä¸‰å…ƒç»„æŠ½å–ã€æ ‡é¢˜æ‰©å†™ã€æ‘˜è¦ç¼©å†™ã€å…³é”®è¯ç”Ÿæˆå’Œæœºå™¨é˜…è¯»ç†è§£ç­‰ä»»åŠ¡ã€‚
ä¸‹é¢å±•ç¤ºäº†TechGPTçš„å‡ ä¸ªæ ¸å¿ƒèƒ½åŠ›ã€‚

1. **å‘½åå®ä½“è¯†åˆ«**
<table>   <colgroup>     <col style="width:60%">     <col style="width:40%">   </colgroup>   <tr>     <td>é—®é¢˜</td>     <td>å›å¤</td> </tr>
  <tr>     <td>æŠ½å–å‡ºä¸‹é¢æ–‡æœ¬çš„å®ä½“å’Œå®ä½“ç±»å‹ï¼š<br/>ã€Šå¥³äººæ ‘ã€‹ï¼Œå›½äº§ç”µè§†å‰§ï¼Œç”±å¯¼æ¼”ç”°è¿ªæ‰§å¯¼ï¼Œæ ¹æ®ä½œå®¶å­é¡µçš„åŸè‘—æ”¹ç¼–ï¼Œæ•…äº‹ä»1947å¹´å¼€å§‹ï¼Œè·¨è¶Šäº†è§£æ”¾æˆ˜äº‰å’Œå»ºå›½åˆæœŸä¸¤å¤§å†å²æ—¶æœŸï¼Œå±•ç°äº†æˆ˜æ–—åœ¨éšå½¢æˆ˜çº¿ä¸Šçš„äººæ°‘è‹±é›„æ˜¯å¦‚ä½•ä¸æƒ§æ€•ä»»ä½•å±é™©ï¼Œä¸è®¡è¾ƒä¸ªäººç‰ºç‰²ï¼Œç”šè‡³ä¸é¡¾äººæ°‘å†…éƒ¨çš„è¯¯è§£å’Œç”Ÿæ­»è£å†³ï¼Œéƒ½ä¸æš´éœ²ä¸ªäººçœŸå®èº«ä»½ï¼Œè‡³æ­»ä¸æ¸ï¼Œä¸æ•Œäººå‘¨æ—‹åˆ°åº•çš„è‹±é›„æ•…äº‹ã€‚</td>     <td>å®ä½“å: å¥³äººæ ‘ï¼Œ å®ä½“ç±»å‹ï¼šå½±è§†ä½œå“<br/>å®ä½“å: ç”°è¿ªï¼Œ å®ä½“ç±»å‹ï¼šäººç‰©<br/>å®ä½“å: å­é¡µï¼Œ å®ä½“ç±»å‹ï¼šäººç‰©<br/>å®ä½“å: 1947å¹´ï¼Œ å®ä½“ç±»å‹ï¼šDate</td> </tr>
</table>

2. **å…³ç³»ä¸‰å…ƒç»„æŠ½å–**
<table>   <colgroup>     <col style="width:60%">     <col style="width:40%">   </colgroup>   <tr>     <td>é—®é¢˜</td>     <td>å›å¤</td> </tr>
  <tr>     <td>
è¯·æŠ½å–å‡ºä¸‹é¢çš„å…³ç³»ä¸‰å…ƒç»„ï¼š<br>é™ˆå¥•è¿…ï¼ˆEason Chanï¼‰ï¼Œ1974å¹´7æœˆ27æ—¥å‡ºç”Ÿäºé¦™æ¸¯ï¼Œä¸­å›½é¦™æ¸¯æµè¡Œä¹ç”·æ­Œæ‰‹ã€æ¼”å‘˜ï¼Œæ¯•ä¸šäºè‹±å›½é‡‘æ–¯é¡¿å¤§å­¦ã€‚     1995å¹´å› è·å¾—ç¬¬14å±Šæ–°ç§€æ­Œå”±å¤§èµ›å† å†›è€Œæ­£å¼å‡ºé“ã€‚2005å¹´å‘è¡Œçš„æ­Œæ›²ã€Šæµ®å¤¸ã€‹æˆä¸ºå…¶æ­Œå”±ç”Ÿæ¶¯çš„ä»£è¡¨ä½œå“ä¹‹ä¸€ã€‚</td>     <td><é™ˆå¥•è¿…, å‡ºç”Ÿæ—¥æœŸ, 1974å¹´><br><é™ˆå¥•è¿…, å‡ºç”Ÿåœ°, é¦™æ¸¯><br>
<é™ˆå¥•è¿…, æ¯•ä¸šé™¢æ ¡, è‹±å›½é‡‘æ–¯é¡¿å¤§å­¦><br>
<é™ˆå¥•è¿…, å›½ç±, ä¸­å›½></td> </tr>
</table>

3. **æ ‡é¢˜æ‰©å†™**
<table>   <colgroup>     <col style="width:60%">     <col style="width:40%">   </colgroup>   <tr>     <td>é—®é¢˜</td>     <td>å›å¤</td> </tr>
  <tr>     <td>è¯·æŠŠä¸‹åˆ—æ ‡é¢˜æ‰©å†™æˆæ‘˜è¦, ä¸å°‘äº100å­—: åŸºäºè§†è§‰è¯­è¨€å¤šæ¨¡æ€çš„å®ä½“å…³ç³»è”åˆæŠ½å–çš„ç ”ç©¶ã€‚</td>     <td>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€çš„å¤šæ¨¡æ€å®ä½“å…³ç³»è”åˆæŠ½å–å‡ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰è”ç³»ï¼Œé€šè¿‡å°†å›¾åƒä¸­çš„ç‰©ä½“ä¸ç›¸åº”çš„æ–‡æœ¬æè¿°è¿›è¡ŒåŒ¹é…æ¥è¯†åˆ«å®ä½“ä¹‹é—´çš„å…³ç³»ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ç»“æ„â€”â€”æ·±åº¦åŒå‘ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œï¼ˆBiDANï¼‰ï¼Œç”¨äºå®ç°è¿™ç§è”åˆæå–ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</td> </tr>
</table>

4. **æ‘˜è¦ç¼©å†™**
<table>   <colgroup>     <col style="width:60%">     <col style="width:40%">   </colgroup>   <tr>     <td>é—®é¢˜</td>     <td>å›å¤</td> </tr>
  <tr>     <td>è¯·æŠŠä¸‹åˆ—æ‘˜è¦ç¼©å†™æˆæ ‡é¢˜:æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€çš„å¤šæ¨¡æ€å®ä½“å…³ç³»è”åˆæŠ½å–å‡ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰è”ç³»ï¼Œé€šè¿‡å°†å›¾åƒä¸­çš„ç‰©ä½“ä¸ç›¸åº”çš„æ–‡æœ¬æè¿°è¿›è¡ŒåŒ¹é…æ¥è¯†åˆ«å®ä½“ä¹‹é—´çš„å…³ç³»ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ç»“æ„â€”â€”æ·±åº¦åŒå‘ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œï¼ˆBiDAFï¼‰ï¼Œç”¨äºå®ç°è¿™ç§è”åˆæå–ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</td>     <td>åŸºäºè§†è§‰è¯­è¨€çš„å¤šæ¨¡æ€å®ä½“å…³ç³»è”åˆæŠ½å–ç ”ç©¶ã€‚</td> </tr>
</table>

5. **å…³é”®è¯ç”Ÿæˆ**
<table>   <colgroup>     <col style="width:60%">     <col style="width:40%">   </colgroup>   <tr>     <td>é—®é¢˜</td>     <td>å›å¤</td> </tr>
  <tr>     <td>è¯·æå–ä¸‹é¢æ–‡æœ¬ä¸­çš„å…³é”®è¯ã€‚<br>æœ¬ä½“æ˜¯ä¸€ç§é‡è¦çš„çŸ¥è¯†åº“,å…¶åŒ…å«çš„ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯å¯ä»¥ä¸ºé—®ç­”ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢ã€è¯­ä¹‰Webã€ä¿¡æ¯æŠ½å–ç­‰é¢†åŸŸçš„ç ”ç©¶åŠç›¸å…³åº”ç”¨æä¾›é‡è¦çš„æ”¯æŒ.å› è€Œ,å¦‚ä½•å¿«é€Ÿæœ‰æ•ˆåœ°æ„å»ºæœ¬ä½“å…·æœ‰éå¸¸é‡è¦çš„ç ”ç©¶ä»·å€¼.ç ”ç©¶è€…ä»¬åˆ†åˆ«ä»ä¸åŒè§’åº¦æå‡ºäº†å¤§é‡æœ‰æ•ˆåœ°è¿›è¡Œæœ¬ä½“æ„å»ºçš„æ–¹æ³•.ä¸€èˆ¬æ¥è®²,è¿™äº›æœ¬ä½“æ„å»ºæ–¹æ³•å¯ä»¥åˆ†ä¸ºæ‰‹å·¥æ„å»ºçš„æ–¹æ³•å’Œé‡‡ç”¨è‡ªåŠ¨ã€åŠè‡ªåŠ¨æŠ€æœ¯æ„å»ºçš„æ–¹æ³•.æ‰‹å·¥æœ¬ä½“çš„æ–¹æ³•å¾€å¾€éœ€è¦æœ¬ä½“ä¸“å®¶å‚ä¸åˆ°æ„å»ºçš„æ•´ä¸ªè¿‡ç¨‹,å­˜åœ¨ç€æ„å»ºæˆæœ¬é«˜ã€æ•ˆç‡ä½ä¸‹ã€ä¸»è§‚æ€§å¼ºã€ç§»æ¤ä¸ä¾¿ç­‰ç¼ºç‚¹,å› è€Œ,æ­¤ç±»æ–¹æ³•æ­£é€æ­¥è¢«å¤§é‡åŸºäºè‡ªåŠ¨ã€åŠè‡ªåŠ¨æŠ€æœ¯çš„æœ¬ä½“æ„å»ºæ–¹æ³•æ‰€ä»£æ›¿.è‡ªåŠ¨ã€åŠè‡ªåŠ¨æ„å»ºçš„æ–¹æ³•ä¸éœ€è¦ï¼ˆæˆ–ä»…éœ€å°‘é‡ï¼‰äººå·¥å‚ä¸,å¯ä»¥å¾ˆæ–¹ä¾¿åœ°ä½¿ç”¨å…¶å®ƒç ”ç©¶é¢†åŸŸï¼ˆå¦‚æœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ï¼‰çš„æœ€æ–°ç ”ç©¶æˆæœ,ä¹Ÿå¯ä»¥æ–¹ä¾¿åœ°ä½¿ç”¨ä¸åŒæ•°æ®æºè¿›è¡Œæœ¬ä½“æ„å»º.</td>     <td>æœ¬ä½“ï¼ŒçŸ¥è¯†åº“ï¼Œé—®ç­”ç³»ç»Ÿï¼Œä¿¡æ¯æ£€ç´¢ï¼Œè¯­ä¹‰ Webï¼Œä¿¡æ¯æŠ½å–ï¼Œç»Ÿè®¡ä¸»å¯¼ï¼Œè¯­è¨€åˆ†æä¸»å¯¼ï¼Œæœ¯è¯­æŠ½å–ï¼Œæ¦‚å¿µæŠ½å–ï¼Œå…³ç³»æŠ½å–ï¼Œæœ¬ä½“å½¢æˆï¼Œè¯„ä»·æ–¹æ³•ï¼Œæœ¬ä½“æ„å»ºç³»ç»Ÿ</td> </tr>
</table>

6. **æœºå™¨é˜…è¯»ç†è§£**
<table>   <colgroup>     <col style="width:60%">     <col style="width:40%">   </colgroup>   <tr>     <td>é—®é¢˜</td>     <td>å›å¤</td> </tr>
  <tr>     <td>è¯·é—®è¿™èµ·äº¤é€šäº‹æ•…æ˜¯è°çš„è´£ä»»å±…å¤š?<br>å°è½¦å’Œæ‘©æ‰˜è½¦å‘ç”Ÿäº‹æ•…ï¼Œåœ¨æ— çº¢ç»¿ç¯çš„åå­—è·¯å£ï¼Œå°åœè½¦çœ‹çœ‹å·¦å³ï¼Œåœ¨è§‰å¾—å®‰å…¨çš„æƒ…å†µä¸‹åˆ¹è½¦æ…¢æ…¢ä»¥æ—¶é€Ÿ10å…¬é‡Œå·¦å³çš„é€Ÿåº¦é å³è¡Œé©¶è¿‡è·¯å£ï¼Œå¥½æ²¡æœ‰å‡ºåˆ°åå­—è·¯å£æ­£ä¸­æ—¶ï¼Œè¢«å·¦è¾¹çªç„¶å¿«é€Ÿè¡Œé©¶è¿‡æ¥çš„æ‘©æ‰˜è½¦æ’åœ¨è½¦å¤´å‰ï¼Œ  æ‘©æ‰˜è½¦ä¸»æ‘”åˆ°è†ç›–å’Œæª«ä¼¤è„¸éƒ¨ï¼Œè¯·é—®è¿™èµ·äº¤é€šäº‹æ•…æ˜¯è°çš„è´£ä»»å±…å¤šã€‚</td> <td>æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š<br>
1. å°è½¦å¸æœºåº”è¯¥å…ˆè§‚å¯Ÿäº¤é€šä¿¡å·ç¯æ˜¯å¦äº®èµ·ï¼Œå¦‚æœæ²¡æœ‰ä¿¡å·ç¯ï¼Œåˆ™åº”å‡é€Ÿæ…¢è¡Œé€šè¿‡äº¤å‰è·¯å£ï¼›<br>2. åœ¨æ²¡æœ‰ä¿¡å·ç¯çš„äº¤å‰å£ä¸­ï¼Œå°è½¦çš„é©¾é©¶å‘˜åº”è¯¥åœ¨ç¡®ä¿å®‰å…¨çš„å‰æä¸‹é€‰æ‹©å³ä¾§è½¦é“è¡Œé©¶ï¼Œä»¥é¿å…ä¸æ¥è‡ªå·¦ä¾§çš„è½¦è¾†ç›¸æ’ï¼›
3. ç”±äºäº‹æ•…å‘ç”Ÿåœ°ç‚¹æ˜¯åå­—è·¯å£ï¼Œå› æ­¤å¯ä»¥ç¡®å®šè¯¥äº‹æ•…å±äºé“è·¯äº¤é€šäº‹æ•…èŒƒç•´ã€‚<br>æ ¹æ®ä»¥ä¸Šåˆ†æï¼Œæˆ‘è®¤ä¸ºè¿™èµ·äº¤é€šäº‹æ•…çš„ä¸»è¦è´£ä»»åœ¨äºæ‘©æ‰˜è½¦é©¾é©¶äººçš„è¡Œä¸ºã€‚å› ä¸ºæ‘©æ‰˜è½¦é©¾é©¶å‘˜åœ¨æ²¡æœ‰ä¿¡å·ç¯çš„æƒ…å†µä¸‹ä»å·¦ä¾§å†²è¿‡æ¥ï¼Œå¯¼è‡´ä¸å°è½¦åœ¨åå­—è·¯å£ç›¸é‡ï¼Œé€ æˆäº†ä¼¤å®³ã€‚
ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦è€ƒè™‘å…¶ä»–å› ç´ çš„å½±å“ï¼Œä¾‹å¦‚å°è½¦çš„é€Ÿåº¦ã€è·¯é¢çŠ¶å†µç­‰ã€‚å¦‚æœå°è½¦çš„è½¦é€Ÿè¿‡å¿«æˆ–è€…è·¯é¢æ¹¿æ»‘ï¼Œä¹Ÿå¯èƒ½å¯¹äº‹æ•…çš„å‘ç”Ÿäº§ç”Ÿå½±å“ã€‚ä½†æ˜¯ï¼Œç”±äºæä¾›çš„ä¿¡æ¯ä¸­æ²¡æœ‰æåˆ°è¿™äº›ç»†èŠ‚ï¼Œæ‰€ä»¥æˆ‘ä»¬åªèƒ½åŸºäºä¸Šè¿°åŸºæœ¬åˆ¤æ–­æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚</td>
  </tr>
</table>

## å‡†å¤‡å·¥ä½œ Preparation

ç”±äºLLaMAå¼€æºåè®®çš„é™åˆ¶ï¼Œæœ¬æ¨¡å‹ä»…é™äºç ”ç©¶å’Œå­¦ä¹ ç”¨é€”ä½¿ç”¨ã€‚è¯·ä¸¥æ ¼éµå®ˆLLaMAçš„ä½¿ç”¨è¦æ±‚å’Œè§„èŒƒã€‚ä¸ºäº†ç¡®ä¿è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦ç¡®è®¤æ‚¨å…·æœ‰LLaMAçš„åŸå§‹æƒé‡ï¼Œå¹¶æ¥è‡ªå®Œå…¨åˆæ³•æ¸ é“ã€‚

---
According to the limitations of the LLaMA open source agreement, this model is limited to research and learning purposes. Please strictly abide by the usage requirements and specifications of LLaMA. To ensure this, we need to confirm that you have LLaMA's original weight and come from a completely legitimate source.

1. ä½ éœ€è¦å…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œå¹¶æ ¡éªŒå®ƒä»¬çš„æ£€æŸ¥å’Œï¼š
```
md5sum ./*
6b2b545ff7bacaeec6297198b4b745dd  ./config.json.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
4ba9cc7f11df0422798971bc962fe076  ./generation_config.json.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
560b35ffd8a7a1f5b2d34a94a523659a  ./pytorch_model.bin.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
85ae4132b11747b1609b8953c7086367  ./special_tokens_map.json.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
953dceae026a7aa88e062787c61ed9b0  ./tokenizer_config.json.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
e765a7740a908b5e166e95b6ee09b94b  ./tokenizer.model.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
```

2. æ ¹æ®è¿™é‡Œâ†’çš„[æŒ‡å®šè„šæœ¬](https://github.com/neukg/TechGPT/blob/main/utils/decrypt.py)è§£ç æ¨¡å‹æƒé‡ï¼š
```shell
for file in $(ls /path/encrypt_weight); do
  python decrypt.py --type decrypt \
    --input_file /path/encrypt_weight/"$file" \
    --output_dir /path/to_finetuned_model \
    --key_file /path/to_original_llama_7B/consolidated.00.pth
done
```

è¯·å°† `/path/encrypt_weight`æ›¿æ¢ä¸ºä½ ä¸‹è½½çš„åŠ å¯†æ–‡ä»¶ç›®å½•ï¼ŒæŠŠ`/path/to_original_llama_7B`æ›¿æ¢ä¸ºä½ å·²æœ‰çš„åˆæ³•LLaMA-7Bæƒé‡ç›®å½•ï¼Œé‡Œé¢åº”è¯¥æœ‰åŸLLaMAæƒé‡æ–‡ä»¶`consolidated.00.pth`ï¼Œå°† `/path/to_finetuned_model` æ›¿æ¢ä¸ºä½ è¦å­˜æ”¾è§£ç åæ–‡ä»¶çš„ç›®å½•ã€‚
åœ¨è§£ç å®Œæˆåï¼Œåº”è¯¥å¯ä»¥å¾—åˆ°ä»¥ä¸‹æ–‡ä»¶ï¼š
```shell
./config.json
./generation_config.json
./pytorch_model.bin
./special_tokens_map.json
./tokenizer_config.json
./tokenizer.model
```

3. è¯·æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶çš„æ£€æŸ¥å’Œæ˜¯å¦å’Œä¸‹é¢ç»™å‡ºçš„ç›¸åŒ, ä»¥ä¿è¯è§£ç å‡ºæ­£ç¡®çš„æ–‡ä»¶ï¼š
```
md5sum ./*
6d5f0d60a6e36ebc1518624a46f5a717  ./config.json
2917a1cafb895cf57e746cfd7696bfe5  ./generation_config.json
0d322cb6bde34f7086791ce12fbf2bdc  ./pytorch_model.bin
15f7a943faa91a794f38dd81a212cb01  ./special_tokens_map.json
08f6f621dba90b2a23c6f9f7af974621  ./tokenizer_config.json
6ffe559392973a92ea28032add2a8494  ./tokenizer.model
```
---
1. Git clone this model first.
```
md5sum ./*
6b2b545ff7bacaeec6297198b4b745dd  ./config.json.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
4ba9cc7f11df0422798971bc962fe076  ./generation_config.json.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
560b35ffd8a7a1f5b2d34a94a523659a  ./pytorch_model.bin.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
85ae4132b11747b1609b8953c7086367  ./special_tokens_map.json.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
953dceae026a7aa88e062787c61ed9b0  ./tokenizer_config.json.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
e765a7740a908b5e166e95b6ee09b94b  ./tokenizer.model.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.enc
```
2. Decrypt the files using the scripts in https://github.com/neukg/TechGPT/blob/main/utils/decrypt.py
You can use the following command in Bash.
Please replace `/path/to_encrypted` with the path where you stored your encrypted file, 
replace `/path/to_original_llama_7B` with the path where you stored your original LLaMA-7B file `consolidated.00.pth`, 
and replace `/path/to_finetuned_model` with the path where you want to save your final trained model.
```bash
for file in $(ls /path/encrypt_weight); do
  python decrypt.py --type decrypt \
    --input_file /path/encrypt_weight/"$file" \
    --output_dir /path/to_finetuned_model \
    --key_file /path/to_original_llama_7B/consolidated.00.pth
done
```
After executing the aforementioned command, you will obtain the following files.
```
./config.json
./generation_config.json
./pytorch_model.bin
./special_tokens_map.json
./tokenizer_config.json
./tokenizer.model
```
3. Check md5sum
You can verify the integrity of these files by performing an MD5 checksum to ensure their complete recovery.
Here are the MD5 checksums for the relevant files:
```
md5sum ./*
6d5f0d60a6e36ebc1518624a46f5a717  ./config.json
2917a1cafb895cf57e746cfd7696bfe5  ./generation_config.json
0d322cb6bde34f7086791ce12fbf2bdc  ./pytorch_model.bin
15f7a943faa91a794f38dd81a212cb01  ./special_tokens_map.json
08f6f621dba90b2a23c6f9f7af974621  ./tokenizer_config.json
6ffe559392973a92ea28032add2a8494  ./tokenizer.model
```

## ä½¿ç”¨æ–¹æ³• Model Usage

è¯·æ³¨æ„åœ¨**è®­ç»ƒ**å’Œ**æ¨ç†**é˜¶æ®µ, æ¨¡å‹æ¥æ”¶çš„è¾“å…¥æ ¼å¼æ˜¯ä¸€è‡´çš„ï¼š

Please note that the input should be formatted as follows in both **training** and **inference**.

``` python
Human: {input} \n\nAssistant:
```

è¯·åœ¨ä½¿ç”¨TechGPTä¹‹å‰ä¿è¯ä½ å·²ç»å®‰è£…å¥½`transfomrers`å’Œ`torch`ï¼š

```shell
pip install transformers
pip install torch
```

- æ³¨æ„ï¼Œå¿…é¡»ä¿è¯å®‰è£…çš„ `transformers` çš„ç‰ˆæœ¬ä¸­å·²ç»æœ‰ `LlamaForCausalLM` ã€‚<br>
- Note that you must ensure that the installed version of `transformers` already has `LlamaForCausalLM`.

[Example:](https://github.com/neukg/TechGPT/blob/main/inference.py)

``` python
from transformers import LlamaTokenizer, AutoModelForCausalLM, AutoConfig, GenerationConfig
import torch

ckpt_path = '/path/to_finetuned_model/'
load_type = torch.float16
device = torch.device(0)
tokenizer = LlamaTokenizer.from_pretrained(ckpt_path)
tokenizer.pad_token_id = 0
tokenizer.bos_token_id = 1
tokenizer.eos_token_id = 2
tokenizer.padding_side = "left"
model_config = AutoConfig.from_pretrained(ckpt_path)
model = AutoModelForCausalLM.from_pretrained(ckpt_path, torch_dtype=load_type, config=model_config)
model.to(device)
model.eval()

prompt = "Human: è¯·æŠŠä¸‹åˆ—æ ‡é¢˜æ‰©å†™æˆæ‘˜è¦, ä¸å°‘äº100å­—: åŸºäºè§†è§‰è¯­è¨€å¤šæ¨¡æ€çš„å®ä½“å…³ç³»è”åˆæŠ½å–çš„ç ”ç©¶ \n\nAssistant: "
inputs = tokenizer(prompt, return_tensors="pt")
input_ids = inputs["input_ids"].to(device)
generation_config = GenerationConfig(
    temperature=0.1,
    top_p=0.75,
    top_k=40,
    num_beams=1,
    bos_token_id=1,
    eos_token_id=2,
    pad_token_id=0,
    max_new_tokens=128,
    min_new_tokens=10,
    do_sample=True,
)
with torch.no_grad():
    generation_output = model.generate(
        input_ids=input_ids,
        generation_config=generation_config,
        return_dict_in_generate=True,
        output_scores=True,
        repetition_penalty=1.2,
    )
    output = generation_output.sequences[0]
    output = tokenizer.decode(output, skip_special_tokens=True)
    print(output)

```

è¾“å‡ºï¼š

```
Human: è¯·æŠŠä¸‹åˆ—æ ‡é¢˜æ‰©å†™æˆæ‘˜è¦, ä¸å°‘äº100å­—: åŸºäºè§†è§‰è¯­è¨€å¤šæ¨¡æ€çš„å®ä½“å…³ç³»è”åˆæŠ½å–çš„ç ”ç©¶ 

Assistant: æ–‡æœ¬ï¼šåŸºäºè§†è§‰è¯­è¨€çš„å¤šæ¨¡æ€çš„å®ä½“å…³ç³»è”åˆæŠ½å–æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦é—®é¢˜ã€‚è¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œ
åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥æå–å›¾åƒä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥è¯†åˆ«å’ŒæŠ½å–å›¾åƒä¸­çš„äººã€ç‰©ã€åœ°ç‚¹ç­‰å®ä½“ä¹‹é—´çš„å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ
è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†å¾ˆå¥½çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚
```

## ä¸»è¦è´¡çŒ®è€… Contributors
æ’åä¸åˆ†å…ˆå
<table>
  <tr>
    <td>
      <img src="https://avatars.githubusercontent.com/u/37218117?v=4" alt="Contributor 1" height="150">
      <br>
      <b>Adaning @ Northeastern University</b>
      <br>
      TechGPTé¡¹ç›®çš„æ ¸å¿ƒå¼€å‘æˆå‘˜
    </td>
    <td>
      <img src="https://avatars.githubusercontent.com/u/38159292?v=4" alt="Contributor 2" height="150">
      <br>
      <b>Asimok @ Northeastern University</b>
      <br>
      TechGPTé¡¹ç›®çš„æ ¸å¿ƒå¼€å‘æˆå‘˜
    </td>
    <td>
      <img src="https://avatars.githubusercontent.com/u/105141916?v=4" alt="Contributor 3" height="150">
      <br>
      <b>abel-nlp @ Northeastern University</b>
      <br>
      TechGPTé¡¹ç›®çš„æ ¸å¿ƒå¼€å‘æˆå‘˜
    </td>
  </tr>
  </table>


## å…è´£å£°æ˜ Disclaimers

è¯¥é¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œç¦æ­¢ç”¨äºå•†ä¸šç”¨é€”ã€‚åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨è€…éœ€è®¤çœŸé˜…è¯»å¹¶éµå®ˆä»¥ä¸‹å£°æ˜:

1. æœ¬é¡¹ç›®ä»…ä¸ºå¤§æ¨¡å‹æµ‹è¯•åŠŸèƒ½è€Œç”Ÿï¼Œä½¿ç”¨è€…éœ€è‡ªè¡Œæ‰¿æ‹…é£é™©å’Œè´£ä»»ï¼Œå¦‚å› ä½¿ç”¨ä¸å½“è€Œå¯¼è‡´çš„ä»»ä½•æŸå¤±æˆ–ä¼¤å®³ï¼Œæœ¬é¡¹ç›®æ¦‚ä¸è´Ÿè´£ã€‚
2. æœ¬é¡¹ç›®ä¸­å‡ºç°çš„ç¬¬ä¸‰æ–¹é“¾æ¥æˆ–åº“ä»…ä¸ºæä¾›ä¾¿åˆ©è€Œå­˜åœ¨ï¼Œå…¶å†…å®¹å’Œè§‚ç‚¹ä¸æœ¬é¡¹ç›®æ— å…³ã€‚ä½¿ç”¨è€…åœ¨ä½¿ç”¨æ—¶éœ€è‡ªè¡Œè¾¨åˆ«ï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…ä»»ä½•è¿å¸¦è´£ä»»ï¼›
3. ä½¿ç”¨è€…åœ¨æµ‹è¯•å’Œä½¿ç”¨æ¨¡å‹æ—¶ï¼Œåº”éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ï¼Œå¦‚å› ä½¿ç”¨ä¸å½“è€Œé€ æˆæŸå¤±çš„ï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…è´£ä»»ï¼Œä½¿ç”¨è€…åº”è‡ªè¡Œæ‰¿æ‹…ï¼›è‹¥é¡¹ç›®å‡ºç°ä»»ä½•é”™è¯¯ï¼Œè¯·å‘æˆ‘æ–¹åé¦ˆï¼Œä»¥åŠ©äºæˆ‘ä»¬åŠæ—¶ä¿®å¤ï¼›
4. æœ¬æ¨¡å‹ä¸­å‡ºç°çš„ä»»ä½•è¿åæ³•å¾‹æ³•è§„æˆ–å…¬åºè‰¯ä¿—çš„å›ç­”ï¼Œå‡ä¸ä»£è¡¨æœ¬é¡¹ç›®è§‚ç‚¹å’Œç«‹åœºï¼Œæˆ‘ä»¬å°†ä¸æ–­å®Œå–„æ¨¡å‹å›ç­”ä»¥ä½¿å…¶æ›´ç¬¦åˆç¤¾ä¼šä¼¦ç†å’Œé“å¾·è§„èŒƒã€‚

ä½¿ç”¨æœ¬é¡¹ç›®å³è¡¨ç¤ºæ‚¨å·²ç»ä»”ç»†é˜…è¯»ã€ç†è§£å¹¶åŒæ„éµå®ˆä»¥ä¸Šå…è´£å£°æ˜ã€‚æœ¬é¡¹ç›®ä¿ç•™åœ¨ä¸é¢„å…ˆé€šçŸ¥ä»»ä½•äººçš„æƒ…å†µä¸‹ä¿®æ”¹æœ¬å£°æ˜çš„æƒåˆ©ã€‚

---

This project is for learning exchange only, commercial use is prohibited. During use, users should carefully read and abide by the following statements:

1. This project is only for the test function of the large model, and the user shall bear the risks and responsibilities. This project shall not be responsible for any loss or injury caused by improper use.
2. The third-party links or libraries appearing in this project exist only for convenience, and their content and opinions have nothing to do with this project. Users need to identify themselves when using it, and this project does not bear any joint and several liabilities;
3. Users should abide by the relevant laws and regulations when testing and using the model. If the loss is caused by improper use, the project will not bear the responsibility, and the user should bear it by themselves; if there is any error in the project, please feedback to us. to help us fix it in a timely manner;
4. Any answers in this model that violate laws and regulations or public order and good customs do not represent the views and positions of this project. We will continue to improve the model answers to make them more in line with social ethics and moral norms.

Using this project means that you have carefully read, understood and agreed to abide by the above disclaimer. The project reserves the right to modify this statement without prior notice to anyone.

## å¼•ç”¨ Citation

å¦‚æœä½¿ç”¨æœ¬é¡¹ç›®çš„ä»£ç ã€æ•°æ®æˆ–æ¨¡å‹ï¼Œè¯·å¼•ç”¨æœ¬é¡¹ç›®ã€‚

Please cite our project when using our code, data or model.

```
@misc{TechGPT,
  author = {Feiliang Ren, Ning An, Qi Ma, Hei Lei},
  title = {TechGPT: Technology-Oriented Generative Pretrained Transformer},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/neukg/TechGPT}},
}
```

**æˆ‘ä»¬å¯¹BELLEçš„å·¥ä½œè¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼**

**Our sincere thanks to BELLE for their work!**

```
@misc{ji2023better,
      title={Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation}, 
      author={Yunjie Ji and Yan Gong and Yong Deng and Yiping Peng and Qiang Niu and Baochang Ma and Xiangang Li},
      year={2023},
      eprint={2304.07854},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{BELLE,
  author = {Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Baochang Ma, Xiangang Li},
  title = {BELLE: Be Everyone's Large Language model Engine},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/LianjiaTech/BELLE}},
}
```
